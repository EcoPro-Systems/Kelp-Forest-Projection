{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd98b54b-89d2-44d7-99fd-ce044afcb362",
   "metadata": {},
   "source": [
    "Extract the metrics of importance \n",
    "\n",
    "e.g. temperature, time, lat, lon, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd4ea8d-e47f-49ba-9f4a-da18c151c451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3514/3514 [00:27<00:00, 126.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/kelp_metrics_27_37.pkl saved\n",
      "dict_keys(['dkelp', 'dkelp_kelp', 'dtemp', 'dtemp_temp', 'dtemp_temp_lag', 'dtemp_temp_lag2', 'kelp', 'temp', 'temp_lag', 'temp_lag2', 'sunlight', 'time', 'dtime', 'lat', 'lon', 'dlat', 'dlon', 'elevation', 'delevation', 'temp_char', 'average_temp', 'slope_dkelp_temp_char', 'slope_dkelp_temp_char_err'])\n"
     ]
    }
   ],
   "source": [
    "from kelp_metrics import main as extract_metrics\n",
    "\n",
    "kelp_data = extract_metrics(lower_lat=27, upper_lat=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19dfb2-1cfe-4563-9b25-6d36bdd55062",
   "metadata": {},
   "source": [
    "## Clean the data and format arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df06896-6303-49cb-a4a2-c70fb7fd723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = {\n",
    "    # name : variable\n",
    "    'Temperature [C]': kelp_data['temp'] - 273.15,\n",
    "    '1Q Lag Temperature [C]': kelp_data['temp_lag'] - 273.15,\n",
    "    '2Q Lag Temperature [C]': kelp_data['temp_lag2'] - 273.15,\n",
    "    'Longitude [deg]': kelp_data['lon'], \n",
    "    'Latitude [deg]': kelp_data['lat'],\n",
    "    #'Elevation [m]': kelp_data['elevation'],\n",
    "    'Sunlight [day]': kelp_data['sunlight']\n",
    "    #'time': kelp_data['time'],\n",
    "}\n",
    "\n",
    "# make arrays\n",
    "X = np.array([features[k] for k in features]).T\n",
    "y = kelp_data['kelp']\n",
    "t = kelp_data['time']\n",
    "\n",
    "# Remove nans from lagged values\n",
    "nanmask = np.isnan(features['1Q Lag Temperature [C]']) | np.isnan(features['2Q Lag Temperature [C]'])\n",
    "X = X[~nanmask]\n",
    "y = y[~nanmask]\n",
    "t = t[~nanmask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8655ba-9cc8-4861-bd7c-5fc12ad1d8c5",
   "metadata": {},
   "source": [
    "## Save data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79863cb-3f9a-49f8-be20-2f9d33ab2856",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "feat_df = {}\n",
    "for i, k in enumerate(features):\n",
    "    feat_df[k] = X[:,i]\n",
    "\n",
    "feat_df['time'] = t\n",
    "feat_df['kelp'] = y\n",
    "\n",
    "# Convert the data to a pandas DataFrame and ensure 'time' is in datetime format\n",
    "df = pd.DataFrame(feat_df)\n",
    "\n",
    "# Save data to disk using pandas to_csv function with column names as the first row\n",
    "df.to_csv('extracted_kelp_27_37.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6476c524-e57c-4c2e-9f9c-b818d8d7e922",
   "metadata": {},
   "source": [
    "## Split data into training/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dae4be4-45fc-464d-8ca3-ec10f8e7f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all the data: (70239, 6)\n",
      "Shape of training: (56191, 6)\n",
      "Shape of testing: (14048, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "print(f\"Shape of all the data: {X.shape}\")\n",
    "\n",
    "# Split data into training and testing sets (e.g., 80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of training: {X_train.shape}\")\n",
    "print(f\"Shape of testing: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3313242-2f8d-42f1-99ea-c4a225ae5eb5",
   "metadata": {},
   "source": [
    "## Fit a regression model using linear least-squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b298d62-70cb-498e-af69-61625456209c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Absolute Error Train: 58.165 m^2\n",
      "Avg. Absolute Error Test: 57.972 m^2\n",
      "Coefficients:\n",
      "  Temperature [C]           : -0.634\n",
      "  1Q Lag Temperature [C]    : -6.871\n",
      "  2Q Lag Temperature [C]    : -2.320\n",
      "  Longitude [deg]           : -4.876\n",
      "  Latitude [deg]            : -12.051\n",
      "  Sunlight [day]            : 167.401\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.090\n",
      "Model:                            OLS   Adj. R-squared:                  0.089\n",
      "Method:                 Least Squares   F-statistic:                     921.0\n",
      "Date:                Fri, 30 Aug 2024   Prob (F-statistic):               0.00\n",
      "Time:                        15:34:30   Log-Likelihood:            -3.3056e+05\n",
      "No. Observations:               56191   AIC:                         6.611e+05\n",
      "Df Residuals:                   56184   BIC:                         6.612e+05\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -48.2448     74.817     -0.645      0.519    -194.886      98.396\n",
      "x1            -0.6344      0.236     -2.692      0.007      -1.096      -0.173\n",
      "x2            -6.8713      0.314    -21.898      0.000      -7.486      -6.256\n",
      "x3            -2.3200      0.212    -10.967      0.000      -2.735      -1.905\n",
      "x4            -4.8759      0.711     -6.862      0.000      -6.269      -3.483\n",
      "x5           -12.0505      0.515    -23.379      0.000     -13.061     -11.040\n",
      "x6           167.4011     10.896     15.364      0.000     146.046     188.756\n",
      "==============================================================================\n",
      "Omnibus:                    30007.493   Durbin-Watson:                   2.010\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           231521.005\n",
      "Skew:                           2.498   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.599   Cond. No.                     2.58e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.58e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Fit OLS regressor on training data\n",
    "res = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "# Predict on training data and compute the average absolute error\n",
    "y_ols_train = res.predict(sm.add_constant(X_train))\n",
    "abs_err_ols_train = np.abs(y_train - y_ols_train).mean()\n",
    "print(f\"Avg. Absolute Error Train: {abs_err_ols_train:.3f} m^2\")\n",
    "\n",
    "# Predict on testing data and compute the average absolute error\n",
    "y_ols_test = res.predict(sm.add_constant(X_test))\n",
    "abs_err_ols_test = np.abs(y_test - y_ols_test).mean()\n",
    "print(f\"Avg. Absolute Error Test: {abs_err_ols_test:.3f} m^2\")\n",
    "\n",
    "# Regression coefficients\n",
    "print(\"Coefficients:\")\n",
    "for feat, coef in zip(features.keys(), res.params[1:]):  # Exclude the constant term\n",
    "    print(f\"  {feat:<25} : {coef:.3f}\")\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3339722f-9729-48dd-b7c4-8fae95c0a935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-48.24484085,  -0.63444264,  -6.87131662,  -2.31995681,\n",
       "        -4.87586215, -12.05053615, 167.40113808])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491e041-4a21-4180-a8c4-9661c6568c00",
   "metadata": {},
   "source": [
    "## analyze correlation metrics for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4eeb8-512c-4032-9b49-156163e0a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "def correlation_tests(x, y, input_name, output_name):\n",
    "    # measure the significance of the correlation\n",
    "    correlations = {'input': input_name, 'output':output_name}\n",
    "\n",
    "    # Pearson's correlation\n",
    "    corr, pval = stats.pearsonr(x, y)\n",
    "    correlations['pearsonr'] = {'corr': round(corr, 3), 'pval': round(pval, 3)}\n",
    "\n",
    "    # Kendall's tau\n",
    "    tau, pval = stats.kendalltau(x, y)\n",
    "    correlations['kendalltau'] = {'tau': round(tau, 3), 'pval': round(pval, 3)}\n",
    "\n",
    "    # Calculate the Spearman rank correlation\n",
    "    corr, pval = stats.spearmanr(x, y)\n",
    "    correlations['spearmanr'] = {'corr': round(corr, 3), 'pval': round(pval, 3)}\n",
    "\n",
    "    # Mann-Kendall\n",
    "    tau, pval = stats.mstats.kendalltau(x, y)\n",
    "    correlations['mann.kendall'] = {'tau': round(tau, 3), 'pval': round(pval, 3)}\n",
    "\n",
    "    # Mutual Information (Regression)\n",
    "    mi = mutual_info_regression(x, y)\n",
    "    correlations['mutual_info_regression'] = {'mi': round(mi[0], 3)}\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# for each feature examine the correlations\n",
    "for i, feat in enumerate(features):\n",
    "    corr = correlation_tests(X[:,i],y, feat, 'Kelp Area [m^2]')\n",
    "    print(json.dumps(corr,indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
